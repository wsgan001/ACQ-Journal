\subsection{Setup}
\label{setup}

We consider six real datasets. The first four datasets (Flickr, DBLP, Tencent, and DBpedia) are static graphs.
For {\it Flickr}~\footnote{\url{https://www.flickr.com/}}~\cite{thomee2015new}, a vertex represents a user, and an edge denotes a ``follow'' relationship between two users. For each vertex, we use the 30 most frequent tags of its associated photos as its keywords.
For {\it DBLP}~\footnote{\url{http://dblp.uni-trier.de/xml/}}, a vertex denotes an author, and an edge is a co-authorship relationship between two authors.
For each author, we use the 20 most frequent keywords from the titles of her publications as her keywords.
In the {\it Tencent} graph provided by the KDD contest 2012~\footnote{\url{http://www.kddcup2012.org/c/kddcup2012-track1}}, a vertex is a person, an organization, or a microblog group. Each edge denotes the friendship between two users. The keyword set of each vertex is extracted from a user's profile. For the {\it DBpedia}~\footnote{\url{http://dbpedia.org/datasets}}, each vertex is an entity, and each edge is the relationship between two entities. The keywords of each entity are extracted by the Stanford Analyzer and Lemmatizer.
Table~\ref{tab:dataset} shows the number of vertices and edges, the $k_{max}$ value, a vertex's average degree $\widehat d$, and its keyword set size $\widehat l$.

{\color{blue}
The remaining two dynamic datasets,
i.e., {\it DFlickr} and {\it Youtube}~\cite{mislove-2009-socialnetworksthesis,mislove-2008-flickr},
are dynamic evolving graphs, which contain the snapshots of graphs as the time goes on.
Note that these two datasets do not have keywords.
Both DFlickr and Youtube datasets are about the user friendship networks on Flickr and Youtube websites respectively.
Each vertex denotes a user and each edge denotes a friendship between two users.
DFlickr contains edges which are inserted and deleted during the evolving process;
while in Youtube, there are only inserted edges as the time goes on.
}

\begin{table}[h]
  \centering
  \small
  \footnotesize \caption {Datasets used in our experiments.}\label{tab:dataset}
  \begin{tabular}{c|r|r|c|c|c}
     \hline
          {\bf Dataset}  & \multicolumn{1}{c|}{\textbf{Vertices}}
                         & \multicolumn{1}{c|}{\textbf{Edges}}
                         & $k_{max}$
                         & \textbf{\emph{{$\widehat d$}}}
                         & \textbf{\emph{{$\widehat l$}}}\\
     \hline\hline
          Flickr         &  581,099      &  9,944,548   &   152   & 17.1   &  9.90 \\
     \hline
          DBLP           &  977,288      &  3,432,273   &   118   &  7.02  &  11.8 \\
     \hline
          Tencent        &  2,320,895    &  50,133,369  &   405   &  43.2  &  6.96 \\
     \hline
          DBpedia        &  8,099,955    &  71,527,515  &    95   &  17.7  &  15.0 \\
     \hline
          DFlickr        &  2,585,569    &  45,676,553  &   600   &  17.6  &  --- \\
     \hline
          Youtube        &  1,881,147    &  9,142,046   &   55   &  4.9  &  --- \\
     \hline
  \end{tabular}
\end{table}

{\color{blue}
To evaluate ACQs, we set the default value of $k$ to 6. The input keyword set $S$ is set to the whole set of keywords contained by the query vertex. For each dataset, we randomly select 300 query vertices with core numbers of 6 or more, which ensures that there is a $k$-core containing each query vertex.
Each data point is the average result for these 300 queries.

To evaluate the index maintenance algorithms, we consider all the six datasets.
For the first four datasets, we randomly select 1,000 vertices and for each of them, we randomly insert and delete one keyword to evaluate the perform keyword update. Meanwhile, we randomly insert and delete five groups of edges, each of which has 100 edges, and their core numbers vary from 5 to 25.
For each of the remaining datasets (DFlickr and Youtube), we first take the snapshots in 100 consecutive days, then divide them into five groups, each of which are in a period of 20 consecutive days, and finally we randomly select 200 records from each group as test edges.
}

We implement all the algorithms in Java, and run experiments on a machine having a quad-core Intel i7-3770 3.40GHz processor, and 32GB of memory, with Ubuntu installed.
We present the effectiveness and efficiency results in Sections~\ref{effectiveness} and~\ref{efficiency}.
